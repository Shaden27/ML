{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74A_Avb9VkRu",
        "outputId": "37065d52-d457-459b-cb39-da11ad5de6b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount='True')\n",
        "import pandas as pd\n",
        "import math\n",
        "import re\n",
        "\n",
        "def vocab(sms_spam, sms_spam_test):\n",
        "  #Data Cleaning\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.replace('\\W', ' ', regex=True) # Removes punctuation\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.lower() # Lowercase\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.split() # Word Split\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.replace('\\W', ' ', regex=True) # Removes punctuation\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.lower() # Lowercase\n",
        "  vocabulary = []\n",
        "  for sms in sms_spam['SMS']:\n",
        "    for word in sms:\n",
        "        vocabulary.append(word)\n",
        "\n",
        "  vocabulary = list(set(vocabulary)) # Gives Unique Vocabulary\n",
        "  return sms_spam, sms_spam_test, vocabulary\n",
        "\n",
        "def bernoulli(sms_spam, vocabulary):\n",
        "  word_counts_per_sms = {unique_word: [0] * len(sms_spam['SMS']) for unique_word in vocabulary}\n",
        "\n",
        "  for index, sms in enumerate(sms_spam['SMS']):\n",
        "    for word in sms:\n",
        "        if word_counts_per_sms[word][index] == 0:\n",
        "          word_counts_per_sms[word][index] += 1\n",
        "\n",
        "  word_counts = pd.DataFrame(word_counts_per_sms)\n",
        "  sms_spam_clean = pd.concat([sms_spam, word_counts], axis=1)\n",
        "  return sms_spam_clean\n",
        "\n",
        "def predict(sms_spam_clean, vocabulary):\n",
        "  # Isolating spam and ham messages first\n",
        "  spam_messages = sms_spam_clean[sms_spam_clean['Label'] == 'spam']\n",
        "  ham_messages = sms_spam_clean[sms_spam_clean['Label'] == 'ham']\n",
        "\n",
        "  # P(Spam) and P(Ham)\n",
        "  p_spam = len(spam_messages) / len(sms_spam_clean)\n",
        "  p_ham = len(ham_messages) / len(sms_spam_clean)\n",
        "\n",
        "  # Number of Words in Spam set\n",
        "  n_words_per_spam_message = spam_messages['SMS'].apply(len)\n",
        "  n_spam = n_words_per_spam_message.sum()\n",
        "\n",
        "  # # Number of Words in Ham set\n",
        "  n_words_per_ham_message = ham_messages['SMS'].apply(len)\n",
        "  n_ham = n_words_per_ham_message.sum()\n",
        "\n",
        "  # Length of Vocabulary\n",
        "  n_vocabulary = len(vocabulary)\n",
        "\n",
        "  # 1-Laplace Smoothing\n",
        "  alpha = 1\n",
        "\n",
        "  # Initiate parameters\n",
        "  parameters_spam = {unique_word:0 for unique_word in vocabulary}\n",
        "  parameters_ham = {unique_word:0 for unique_word in vocabulary}\n",
        "\n",
        "  # Calculate parameters\n",
        "  for word in vocabulary:\n",
        "    n_word_given_spam = spam_messages[word].sum() \n",
        "    p_word_given_spam = (n_word_given_spam + alpha) / (n_spam + alpha * n_vocabulary)\n",
        "    parameters_spam[word] = math.log(p_word_given_spam) # Probability of work in ham\n",
        "\n",
        "    n_word_given_ham = ham_messages[word].sum() # ham_messages already defined\n",
        "    p_word_given_ham = (n_word_given_ham + alpha) / (n_ham + alpha * n_vocabulary)\n",
        "    parameters_ham[word] = math.log(p_word_given_ham) # Probability of work in ham\n",
        "\n",
        "  return p_ham, p_spam, parameters_ham, parameters_spam\n",
        "\n",
        "# Appending the test set with predictions\n",
        "def classify_test_set(message, p_ham, p_spam, parameters_ham, parameters_spam): \n",
        "  message = re.sub('\\W', ' ', message)\n",
        "  message = message.lower().split()\n",
        "\n",
        "  p_spam_given_message = p_spam\n",
        "  p_ham_given_message = p_ham\n",
        "\n",
        "  for word in message:\n",
        "      if word in parameters_spam:\n",
        "        p_spam_given_message += parameters_spam[word]\n",
        "\n",
        "      if word in parameters_ham:\n",
        "        p_ham_given_message += parameters_ham[word]\n",
        "\n",
        "  if p_ham_given_message > p_spam_given_message:\n",
        "      return 'ham'\n",
        "  elif p_spam_given_message > p_ham_given_message:\n",
        "      return 'spam'\n",
        "  elif p_spam_given_message == p_ham_given_message:\n",
        "      return 'Equal Probability'\n",
        "\n",
        "def metric(sms_spam_test):\n",
        "  # Performance Metrics\n",
        "  tp = 0\n",
        "  tn = 0\n",
        "  fp = 0\n",
        "  fn = 0\n",
        "  total = sms_spam_test.shape[0]\n",
        "\n",
        "  for row in sms_spam_test.iterrows():\n",
        "    row = row[1]\n",
        "    if row['Label'] == row['predicted'] and row['Label'] == 'spam':\n",
        "        tp += 1\n",
        "    if row['Label'] == row['predicted'] and row['Label'] == 'ham':\n",
        "        tn += 1\n",
        "    if row['Label'] != row['predicted'] and row['Label'] == 'spam':\n",
        "        fp += 1\n",
        "    if row['Label'] != row['predicted'] and row['Label'] == 'ham':\n",
        "        fn += 1\n",
        "  accuracy = (tp+tn)/total\n",
        "  precision = (tp)/(tp+fp)\n",
        "  recall = (tp)/(tp+fn)\n",
        "  f1_score = (2 * precision *recall)/(precision + recall)\n",
        "\n",
        "  print('Accuracy:', accuracy)\n",
        "  print('Precision:', precision)\n",
        "  print('Recall:', recall)\n",
        "  print('F1 Score:', f1_score)\n",
        "\n",
        "def model(sms_spam, sms_spam_test):\n",
        "  sms_spam, sms_spam_test, vocabulary = vocab(sms_spam, sms_spam_test)\n",
        "  sms_spam_clean = bernoulli(sms_spam, vocabulary)\n",
        "  p_ham, p_spam, parameters_ham, parameters_spam = predict(sms_spam_clean, vocabulary)\n",
        "  sms_spam_test['predicted'] = sms_spam_test['SMS'].apply(classify_test_set, args=(p_ham, p_spam, parameters_ham, parameters_spam)) \n",
        "  metric(sms_spam_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enron1 Dataset\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron1train.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron1test.csv', header=None, names=['Label', 'SMS'])\n",
        "model(sms_spam, sms_spam_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcz8I6IISMBL",
        "outputId": "771c347a-1af7-4c5e-8847-3eca6abf1617"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9451754385964912\n",
            "Precision: 0.8926174496644296\n",
            "Recall: 0.9366197183098591\n",
            "F1 Score: 0.9140893470790378\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enron4 Dataset\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron4train.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron4test.csv', header=None, names=['Label', 'SMS'])\n",
        "model(sms_spam, sms_spam_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ODG6CUwQuDnl",
        "outputId": "3e3fa440-f97c-44e4-a888-274e9ee64953"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9502762430939227\n",
            "Precision: 0.9667519181585678\n",
            "Recall: 0.9642857142857143\n",
            "F1 Score: 0.9655172413793103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HW Dataset\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/hwtrain.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/hwtest.csv', header=None, names=['Label', 'SMS'])\n",
        "model(sms_spam, sms_spam_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bywR3TshuGA2",
        "outputId": "5d09c541-3bb6-4951-e853-c92be6ba1039"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9497907949790795\n",
            "Precision: 0.8846153846153846\n",
            "Recall: 0.9274193548387096\n",
            "F1 Score: 0.9055118110236221\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kSRULmQVuLFc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}