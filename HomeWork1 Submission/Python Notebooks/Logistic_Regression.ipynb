{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blotk7dez9dp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc966f1e-2504-404e-da4c-e9631e7496e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount='True')\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_prep_bernoulli(sms_spam, sms_spam_test):\n",
        "  #Data Cleaning\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.replace('\\W', ' ', regex=True) # Removes punctuation\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.lower() # Lowercase\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.split() # Word Split\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.replace('\\W', ' ', regex=True) # Removes punctuation\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.lower() # Lowercase\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.split()\n",
        "  sms_spam['Label'] = sms_spam['Label'].replace({'ham': 0, 'spam': 1})\n",
        "  sms_spam_test['Label'] = sms_spam_test['Label'].replace({'ham': 0, 'spam': 1})\n",
        "  vocabulary = []\n",
        "  for sms in sms_spam['SMS']:\n",
        "    for word in sms:\n",
        "        vocabulary.append(word)\n",
        "\n",
        "  vocabulary = list(set(vocabulary)) # Gives Unique Vocabulary\n",
        "\n",
        "  word_counts_per_sms = {unique_word: [0] * len(sms_spam['SMS']) for unique_word in vocabulary}\n",
        "  word_counts_per_sms_spam = {unique_word: [0] * len(sms_spam_test['SMS']) for unique_word in vocabulary}\n",
        "\n",
        "  for index, sms in enumerate(sms_spam['SMS']):\n",
        "    for word in sms:\n",
        "        if word_counts_per_sms[word][index] == 0:\n",
        "          word_counts_per_sms[word][index] += 1\n",
        "\n",
        "  word_counts = pd.DataFrame(word_counts_per_sms)\n",
        "  sms_spam_clean = pd.concat([sms_spam, word_counts], axis=1)\n",
        "\n",
        "  for index1, sms1 in enumerate(sms_spam_test['SMS']):\n",
        "    for word1 in sms1:\n",
        "      if word1 in word_counts_per_sms_spam:\n",
        "        if word_counts_per_sms_spam[word1][index1] == 0:\n",
        "          word_counts_per_sms_spam[word1][index1] += 1\n",
        "\n",
        "  word_counts_spam = pd.DataFrame(word_counts_per_sms_spam)\n",
        "  train_set_X = word_counts.to_numpy()\n",
        "  train_set_X = train_set_X.reshape(train_set_X.shape[0], -1).T\n",
        "  train_set_Y = sms_spam['Label'].to_numpy()\n",
        "  train_set_Y = train_set_Y.reshape(train_set_Y.shape[0], -1).T\n",
        "  test_set_X = word_counts_spam.to_numpy()\n",
        "  test_set_X = test_set_X.reshape(test_set_X.shape[0], -1).T\n",
        "  test_set_Y = sms_spam_test['Label'].to_numpy()\n",
        "  test_set_Y = test_set_Y.reshape(test_set_Y.shape[0], -1).T\n",
        "\n",
        "  return sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y\n",
        "\n",
        "def data_prep_bow(sms_spam, sms_spam_test):\n",
        "  #Data Cleaning\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.replace('\\W', ' ', regex=True) # Removes punctuation\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.lower() # Lowercase\n",
        "  sms_spam['SMS'] = sms_spam['SMS'].str.split() # Word Split\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.replace('\\W', ' ', regex=True) # Removes punctuation\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.lower() # Lowercase\n",
        "  sms_spam_test['SMS'] = sms_spam_test['SMS'].str.split()\n",
        "  sms_spam['Label'] = sms_spam['Label'].replace({'ham': 0, 'spam': 1})\n",
        "  sms_spam_test['Label'] = sms_spam_test['Label'].replace({'ham': 0, 'spam': 1})\n",
        "  vocabulary = []\n",
        "  for sms in sms_spam['SMS']:\n",
        "    for word in sms:\n",
        "        vocabulary.append(word)\n",
        "\n",
        "  vocabulary = list(set(vocabulary)) # Gives Unique Vocabulary\n",
        "\n",
        "  word_counts_per_sms = {unique_word: [0] * len(sms_spam['SMS']) for unique_word in vocabulary}\n",
        "  word_counts_per_sms_spam = {unique_word: [0] * len(sms_spam_test['SMS']) for unique_word in vocabulary}\n",
        "\n",
        "  for index, sms in enumerate(sms_spam['SMS']):\n",
        "    for word in sms:\n",
        "      word_counts_per_sms[word][index] += 1\n",
        "\n",
        "  word_counts = pd.DataFrame(word_counts_per_sms)\n",
        "  sms_spam_clean = pd.concat([sms_spam, word_counts], axis=1)\n",
        "\n",
        "  for index1, sms1 in enumerate(sms_spam_test['SMS']):\n",
        "    for word1 in sms1:\n",
        "      if word1 in word_counts_per_sms_spam:\n",
        "        word_counts_per_sms_spam[word1][index1] += 1\n",
        "\n",
        "  word_counts_spam = pd.DataFrame(word_counts_per_sms_spam)\n",
        "  train_set_X = word_counts.to_numpy()\n",
        "  train_set_X = train_set_X.reshape(train_set_X.shape[0], -1).T\n",
        "  train_set_Y = sms_spam['Label'].to_numpy()\n",
        "  train_set_Y = train_set_Y.reshape(train_set_Y.shape[0], -1).T\n",
        "  test_set_X = word_counts_spam.to_numpy()\n",
        "  test_set_X = test_set_X.reshape(test_set_X.shape[0], -1).T\n",
        "  test_set_Y = sms_spam_test['Label'].to_numpy()\n",
        "  test_set_Y = test_set_Y.reshape(test_set_Y.shape[0], -1).T\n",
        "\n",
        "  return sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y\n",
        "\n",
        "def sigmoid(z):\n",
        "\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def initialize_with_zeros(dim):\n",
        "  \n",
        "    w = np.zeros((dim, 1))\n",
        "    b = 0\n",
        "    \n",
        "    return w, b\n",
        "\n",
        "def optimize(w, b, X, Y, num_iterations, learning_rate, reg, print_cost = False):\n",
        "\n",
        "    costs = []   \n",
        "    for i in range(num_iterations):\n",
        "        # Cost and gradient calculation\n",
        "        grads, cost = propagate(w,b,X,Y,reg)\n",
        "        # Retrieve derivatives from grads\n",
        "        dw = grads[\"dw\"]\n",
        "        db = grads[\"db\"]\n",
        "        # update \n",
        "        w = w - learning_rate * dw \n",
        "        b = b - learning_rate * db\n",
        "        # Record the costs\n",
        "        if i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "        # Print the cost every 100 training iterations\n",
        "        #if print_cost and i % 100 == 0:\n",
        "            #print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "    params = {\"w\": w,\"b\": b}\n",
        "    grads = {\"dw\": dw,\"db\": db}\n",
        "    \n",
        "    return params, grads, costs\n",
        "\n",
        "def predict(w, b, X, Y):\n",
        "\n",
        "    tp = 0\n",
        "    tn = 0\n",
        "    fp = 0\n",
        "    fn = 0\n",
        "    m = X.shape[1]\n",
        "    Y_prediction = np.zeros((1,m))\n",
        "    w = w.reshape(X.shape[0], 1)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)\n",
        "    \n",
        "    for i in range(A.shape[1]):\n",
        "      \n",
        "        if A[0,i] >= 0.5:\n",
        "            Y_prediction[0,i] = 1\n",
        "            if Y_prediction[0,i] == Y[0][i]:\n",
        "              tp += 1\n",
        "            if Y_prediction[0,i] != Y[0][i]:\n",
        "              fp += 1\n",
        "        else:\n",
        "            Y_prediction[0,i] = 0\n",
        "            if Y_prediction[0,i] == Y[0][i]:\n",
        "              tn += 1\n",
        "            if Y_prediction[0,i] != Y[0][i]:\n",
        "              fn += 1\n",
        "    \n",
        "    return Y_prediction, tp, tn, fp, fn\n",
        "\n",
        "def model(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, reg, print_cost = False):\n",
        "\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    parameters, grads, costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,reg,print_cost)\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    Y_prediction_test, tp, tn, fp, fn = predict(w,b,X_test, Y_test)\n",
        "    accuracy = (tp+tn)/Y_test.shape[1]\n",
        "    precision = (tp)/(tp+fp)\n",
        "    recall = (tp)/(tp+fn)\n",
        "    f1_score = (2 * precision *recall)/(precision + recall)\n",
        "    # Print train/test Errors\n",
        "    d = {\"costs\": costs,\n",
        "         #\"Y_prediction_test\": Y_prediction_test, \n",
        "         #\"Y_prediction_train\" : Y_prediction_train, \n",
        "         \"w\" : w, \n",
        "         \"b\" : b,\n",
        "         \"learning_rate\" : learning_rate,\n",
        "         \"num_iterations\": num_iterations}\n",
        "    print('Accuracy:', accuracy)\n",
        "    print('Precision:', precision)\n",
        "    print('Recall:', recall)\n",
        "    print('F1 Score:', f1_score)\n",
        "    return d\n",
        "\n",
        "def propagate(w, b, X, Y, reg):\n",
        "    \n",
        "    m = X.shape[1]\n",
        "    # FORWARD PROPAGATION (FROM X TO COST)\n",
        "    A = sigmoid(np.dot(w.T, X) + b)  # compute activation\n",
        "    cost = (-1/m) * np.sum((Y * np.log(A)) + (1 - Y) * np.log(1 - A), axis = 1) + (reg/(2*m))*(np.linalg.norm(w, ord=2)) # compute cost and regularize\n",
        "    # BACKWARD PROPAGATION (TO FIND GRAD)\n",
        "    dw = (1/m) * np.dot(X, (A-Y).T) + (reg)*(np.linalg.norm(w, ord=2))/m\n",
        "    db = (1/m) * np.sum(A - Y, axis = 1)\n",
        "    cost = np.squeeze(cost)\n",
        "    \n",
        "    grads = {\"dw\": dw,\n",
        "             \"db\": db}\n",
        "    #print(\"Cost = \",cost)\n",
        "    #print(\"Reg = \",(reg)*(np.square(w))/m)\n",
        "    return grads, cost\n",
        "\n",
        "def tuning(X_train, Y_train, X_test, Y_test, num_iterations, learning_rate, reg, print_cost = False):\n",
        "\n",
        "    w, b = initialize_with_zeros(X_train.shape[0])\n",
        "    parameters, grads, costs = optimize(w,b,X_train,Y_train,num_iterations,learning_rate,reg,print_cost)\n",
        "    w = parameters[\"w\"]\n",
        "    b = parameters[\"b\"]\n",
        "    Y_prediction_test, tp, tn, fp, fn = predict(w,b,X_test, Y_test)\n",
        "    accuracy = (tp+tn)/Y_test.shape[1]\n",
        "    return accuracy\n",
        "\n",
        "def cross_valdation(sms_spam_clean, test_set_X, test_set_Y):\n",
        "  my_dict = {}\n",
        "  #Cross Validation\n",
        "  validation_data = sms_spam_clean.sample(frac=0.3, random_state=25)\n",
        "  validation_data = validation_data.drop('SMS', axis = 1)\n",
        "  validation_label = validation_data['Label']\n",
        "  validation_data = validation_data.drop('Label', axis = 1)\n",
        "  validation_data = validation_data.to_numpy()\n",
        "  validation_data = validation_data.reshape(validation_data.shape[0], -1).T\n",
        "  validation_label = validation_label.to_numpy()\n",
        "  validation_label = validation_label.reshape(validation_label.shape[0], -1).T\n",
        "  reg_param = [0.001,  0.003,  0.005, 0.01,  0.03,  0.05, 0.1, 0.3, 0.5, 1]\n",
        "  for param in reg_param:\n",
        "    accuracy = tuning(validation_data, validation_label, test_set_X, test_set_Y, num_iterations = 1000, learning_rate = 0.005,reg = param, print_cost = True)\n",
        "    my_dict.update({param : accuracy})\n",
        "  print(max(my_dict, key=my_dict.get))\n",
        "  return max(my_dict, key=my_dict.get)"
      ],
      "metadata": {
        "id": "6z05v7xWiqSl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HW Dataset Bernoulli\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/hwtrain.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/hwtest.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y = data_prep_bernoulli(sms_spam, sms_spam_test)\n",
        "reg_param = cross_valdation(sms_spam_clean, test_set_X, test_set_Y)\n",
        "d = model(train_set_X, train_set_Y, test_set_X, test_set_Y, num_iterations = 2000, learning_rate = 0.005,reg = reg_param, print_cost = True)"
      ],
      "metadata": {
        "id": "XAaNjniPQ5Wo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8126d4c7-65b4-4b79-bc43-4b2f81342d9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n",
            "Accuracy: 0.895397489539749\n",
            "Precision: 0.925531914893617\n",
            "Recall: 0.6692307692307692\n",
            "F1 Score: 0.7767857142857142\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HW Dataset Bag of Words\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/hwtrain.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/hwtest.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y = data_prep_bow(sms_spam, sms_spam_test)\n",
        "reg_param = cross_valdation(sms_spam_clean, test_set_X, test_set_Y)\n",
        "d = model(train_set_X, train_set_Y, test_set_X, test_set_Y, num_iterations = 2000, learning_rate = 0.005,reg = reg_param, print_cost = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tJawdO9kr65",
        "outputId": "cbb44727-73b2-4501-ad5e-59448519020c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n",
            "Accuracy: 0.9016736401673641\n",
            "Precision: 0.8672566371681416\n",
            "Recall: 0.7538461538461538\n",
            "F1 Score: 0.8065843621399177\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enron1 Dataset Bernoulli\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron1train.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron1test.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y = data_prep_bernoulli(sms_spam, sms_spam_test)\n",
        "reg_param = cross_valdation(sms_spam_clean, test_set_X, test_set_Y)\n",
        "d = model(train_set_X, train_set_Y, test_set_X, test_set_Y, num_iterations = 2000, learning_rate = 0.005,reg = reg_param, print_cost = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5sbuHJypKRS",
        "outputId": "c98f37c1-7d9a-462d-db67-5b434b59abf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n",
            "Accuracy: 0.9276315789473685\n",
            "Precision: 0.9461538461538461\n",
            "Recall: 0.825503355704698\n",
            "F1 Score: 0.881720430107527\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enron1 Dataset Bag of Words\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron1train.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron1test.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y = data_prep_bow(sms_spam, sms_spam_test)\n",
        "reg_param = cross_valdation(sms_spam_clean, test_set_X, test_set_Y)\n",
        "d = model(train_set_X, train_set_Y, test_set_X, test_set_Y, num_iterations = 2000, learning_rate = 0.005,reg = reg_param, print_cost = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Tx1UlW8vqKs",
        "outputId": "1b2d7550-d7a0-46f1-ee75-d45df3fda416"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.01\n",
            "Accuracy: 0.9232456140350878\n",
            "Precision: 0.9253731343283582\n",
            "Recall: 0.8322147651006712\n",
            "F1 Score: 0.8763250883392225\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enron4 Dataset Bernoulli\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron4train.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron4test.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y = data_prep_bernoulli(sms_spam, sms_spam_test)\n",
        "reg_param = cross_valdation(sms_spam_clean, test_set_X, test_set_Y)\n",
        "d = model(train_set_X, train_set_Y, test_set_X, test_set_Y, num_iterations = 2000, learning_rate = 0.005,reg = reg_param, print_cost = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RGia-WGmxYnJ",
        "outputId": "652557f9-a471-4cf9-86f7-f162b705f6b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.3\n",
            "Accuracy: 0.9337016574585635\n",
            "Precision: 0.9156908665105387\n",
            "Recall: 1.0\n",
            "F1 Score: 0.9559902200488998\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Enron1 Dataset Bag of Words\n",
        "sms_spam = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron4train.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_test = pd.read_csv('/content/drive/MyDrive/Datasets/ML/enron4test.csv', header=None, names=['Label', 'SMS'])\n",
        "sms_spam_clean, train_set_X, train_set_Y, test_set_X, test_set_Y = data_prep_bow(sms_spam, sms_spam_test)\n",
        "reg_param = cross_valdation(sms_spam_clean, test_set_X, test_set_Y)\n",
        "d = model(train_set_X, train_set_Y, test_set_X, test_set_Y, num_iterations = 2000, learning_rate = 0.005,reg = reg_param, print_cost = True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUrgGWOax413",
        "outputId": "9b4b44b0-de48-4210-f427-a9fba073fab9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.001\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:187: RuntimeWarning: divide by zero encountered in log\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:187: RuntimeWarning: invalid value encountered in multiply\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9429097605893186\n",
            "Precision: 0.9285714285714286\n",
            "Recall: 0.9974424552429667\n",
            "F1 Score: 0.9617755856966709\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IX4imzHHzzNB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}